{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../src/models/model_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../src/models/model_utils.py\n",
    "# %load ../../src/models/model_utils.py\n",
    "# %%writefile ../../src/models/model_utils.py\n",
    "\"\"\"\n",
    "Author: Jim Clauwaert\n",
    "Created in the scope of my PhD\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "def CalculateAUC(pred, true):\n",
    "    \"\"\" \n",
    "    Calculate AUC given both the true and predicted labels of a dataset\n",
    "    \n",
    "    INPUT\n",
    "    ------   \n",
    "    pred: np.array\n",
    "        Predicted labels\n",
    "        \n",
    "    true: np.array\n",
    "        True labels\n",
    "        \n",
    "    OUTPUT\n",
    "    -------   \n",
    "    auc: scalar\n",
    "        AUC\n",
    "    \"\"\"\n",
    "    \n",
    "    fpr, tpr, tresholds = metrics.roc_curve(true, pred)\n",
    "    auc = metrics.auc(fpr,tpr)\n",
    "    \n",
    "    return auc\n",
    "    \n",
    "\n",
    "def variable_on_cpu(name, shape, initializer):\n",
    "    \"\"\"Helper to create a Variable stored on CPU memory.\n",
    "    Args:\n",
    "    name: name of the variable\n",
    "    shape: list of ints\n",
    "    initializer: initializer for Variable\n",
    "    Returns:\n",
    "    Variable Tensor\n",
    "    \"\"\"\n",
    "    with tf.device('/cpu:0'):\n",
    "        dtype = tf.float32\n",
    "        var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype)\n",
    "    return var\n",
    "\n",
    "def variable_with_weight_decay(name, shape, stddev, wd):\n",
    "    \"\"\"Helper to create an initialized Variable with weight decay.\n",
    "    Note that the Variable is initialized with a truncated normal distribution.\n",
    "    A weight decay is added only if one is specified.\n",
    "    Args:\n",
    "    name: name of the variable\n",
    "    shape: list of ints\n",
    "    stddev: standard deviation of a truncated Gaussian\n",
    "    wd: add L2Loss weight decay multiplied by this float. If None, weight\n",
    "        decay is not added for this Variable.\n",
    "    Returns:\n",
    "    Variable Tensor\n",
    "    \"\"\"\n",
    "    dtype = tf.float32\n",
    "    var = variable_on_cpu(name, shape, \n",
    "                           tf.truncated_normal_initializer(stddev=stddev, dtype=dtype))\n",
    "    if wd is not None:\n",
    "        w_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "        tf.add_to_collection('losses', w_decay)\n",
    "    return var\n",
    "\n",
    "def par_conv_split(x, keep_prob, motifs, motif_length, stdev, stdev_out, w_decay, \n",
    "                   w_out_decay, num_classes=2, padding=False, extra_layer=False):\n",
    "    \n",
    "    x_image = tf.reshape(x, [-1,1,50,4])\n",
    "    padding = motif_length//2 if padding is True else 0\n",
    "    pool_list = [] \n",
    "    par_conv = math.ceil(50/motif_length)\n",
    "    for conv in range(par_conv):\n",
    "        i_start = (50-(conv+1)*motif_length)-padding\n",
    "        i_end = (50-conv*motif_length)+padding\n",
    "        if i_start < 0:\n",
    "            i_start = 0\n",
    "        x_sub_image_length = len(range(i_start,i_end))\n",
    "        x_sub_image = x_image[:,:,i_start:i_end,:]\n",
    "        with tf.variable_scope('conv{}'.format(conv)) as scope:    \n",
    "            kernel = variable_with_weight_decay('weights',\n",
    "                                             shape=[1, motif_length, 4, motifs],\n",
    "                                             stddev=stdev,\n",
    "                                             wd=w_decay)\n",
    "            conv_unit = tf.nn.conv2d(x_sub_image, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = variable_on_cpu('biases', [motifs], tf.constant_initializer(0.001))\n",
    "            pre_activation = tf.nn.bias_add(conv_unit, biases)\n",
    "            conv_act = tf.nn.relu(pre_activation, name=scope.name)\n",
    "\n",
    "        with tf.variable_scope('pool{}'.format(conv)) as scope:\n",
    "            pool_unit = tf.nn.max_pool(conv_act, ksize=[1, 1, x_sub_image_length, 1], \n",
    "                        strides=[1, 1, x_sub_image_length, 1], padding='SAME')\n",
    "            pool_flat = tf.reshape(pool_unit, [-1, motifs])\n",
    "            pool_list.append(pool_flat)\n",
    "            \n",
    "    layer2 = tf.reshape(tf.concat(pool_list,1),[-1, motifs*par_conv])\n",
    "    \n",
    "    if extra_layer is True:\n",
    "        with tf.variable_scope('fully_connected'):\n",
    "            weights = variable_with_weight_decay('weights', shape=[motifs*par_conv, motifs*par_conv],\n",
    "                                              stddev=stdev_out, wd=w_out_decay)\n",
    "            biases = variable_on_cpu('biases', motifs*par_conv, tf.constant_initializer(0.1))\n",
    "            layer2 = tf.nn.relu(tf.matmul(layer2, weights) + biases)\n",
    "\n",
    "    with tf.variable_scope('out') as scope:\n",
    "        weights = variable_with_weight_decay('weights', shape=[motifs*par_conv, num_classes],\n",
    "                                          stddev=stdev_out, wd=w_out_decay)\n",
    "        biases = variable_on_cpu('biases', num_classes, tf.constant_initializer(0))\n",
    "        softmax_linear = tf.nn.sigmoid(tf.matmul(layer2, weights) + biases)\n",
    "\n",
    "        return softmax_linear\n",
    "\n",
    "def conv_network(x, keep_prob, motifs, motif_length, stdev, stdev_out, w_decay, \n",
    "                 w_out_decay, single_pool=True, num_classes=2, padding=False,\n",
    "                 extra_layer=False):\n",
    "    \n",
    "    x_image = tf.reshape(x, [-1,1,50,4])\n",
    "    padding = motif_length//2 if padding is True else 0\n",
    "    pool_list = [] \n",
    "    motifs = math.ceil(50/motif_length)*motifs\n",
    "    flat_layer = (motifs*(math.ceil(50/motif_length)))\n",
    "\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        kernel = variable_with_weight_decay('weights',\n",
    "                                             shape=[1, motif_length, 4, motifs],\n",
    "                                             stddev=stdev,\n",
    "                                             wd=w_decay)\n",
    "        conv = tf.nn.conv2d(x_image, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = variable_on_cpu('biases', [motifs], tf.constant_initializer(0))\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv1 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "    \n",
    "    if single_pool == True:\n",
    "        pool1 = tf.nn.max_pool(conv1, ksize=[1, 1, 50, 1], strides=[1, 1, 50, 1],\n",
    "                             padding='SAME', name='pool1')\n",
    "        layer2 = tf.reshape(pool1, [-1, motifs])\n",
    "        full_connect = motifs\n",
    "    if single_pool == False:\n",
    "        pool1 = tf.nn.max_pool(conv1, ksize=[1, 1, motif_length, 1], strides=[1, 1, motif_length, 1],\n",
    "                             padding='SAME', name='pool1')\n",
    "        layer2 = tf.reshape(pool1, [-1, flat_layer])\n",
    "        full_connect = flat_layer\n",
    "    \n",
    "    if extra_layer is True:\n",
    "        with tf.variable_scope('fully_connected'):\n",
    "            weights = variable_with_weight_decay('weights', shape=[full_connect, full_connect],\n",
    "                                              stddev=stdev_out, wd=w_out_decay)\n",
    "            biases = variable_on_cpu('biases', full_connect, tf.constant_initializer(0.1))\n",
    "            fully_connected = tf.nn.relu(tf.matmul(layer2, weights) + biases)\n",
    "            layer2 = tf.nn.dropout(fully_connected, keep_prob)\n",
    "\n",
    "    with tf.variable_scope('out') as scope:\n",
    "        weights = variable_with_weight_decay('weights', [full_connect, num_classes],\n",
    "                                              stddev=stdev_out, wd=w_out_decay)\n",
    "        biases = variable_on_cpu('biases', [num_classes],\n",
    "                                  tf.constant_initializer(0))\n",
    "        softmax_linear = tf.sigmoid(tf.matmul(layer2, weights) + biases)\n",
    "\n",
    "\n",
    "    return softmax_linear\n",
    "    \n",
    "\n",
    "def SelectModel(model_label, x, keep_prob, motifs, motif_length, stdev, stdev_out, w_decay, \n",
    "                w_out_decay, num_classes=2, padding=False, extra_layer=False):\n",
    "    \n",
    "    if model_label == \"MS1\":\n",
    "        model = conv_network(x, keep_prob, motifs, motif_length, stdev, stdev_out, w_decay,\n",
    "                             w_out_decay, False, num_classes, padding, extra_layer)\n",
    "    if model_label == \"MS2\":\n",
    "        model = conv_network(x, keep_prob, motifs, motif_length, stdev, stdev_out, w_decay, \n",
    "                             w_out_decay, True, num_classes, padding, extra_layer)\n",
    "    \n",
    "    if model_label == \"MS3\":\n",
    "        model = par_conv_split(x, keep_prob, motifs, motif_length, stdev, stdev_out, w_decay,\n",
    "                               w_out_decay, num_classes, padding, extra_layer)\n",
    "        \n",
    "    return model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
