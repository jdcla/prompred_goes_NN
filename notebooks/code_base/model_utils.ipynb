{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3, 12,  2,  3,  4])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.concatenate(([1,2,3],[12,2,3,4]),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../src/models/model_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../src/models/model_utils.py\n",
    "# %load ../../src/models/model_utils.py\n",
    "# %%writefile ../../src/models/model_utils.py\n",
    "\"\"\"\n",
    "Author: Jim Clauwaert\n",
    "Created in the scope of my PhD\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "def CalculateAUC(pred, true):\n",
    "    \"\"\" \n",
    "    Calculate AUC given both the true and predicted labels of a dataset\n",
    "    \n",
    "    INPUT\n",
    "    ------   \n",
    "    pred: np.array\n",
    "        Predicted labels\n",
    "        \n",
    "    true: np.array\n",
    "        True labels\n",
    "        \n",
    "    OUTPUT\n",
    "    -------   \n",
    "    auc: scalar\n",
    "        AUC\n",
    "    \"\"\"\n",
    "    \n",
    "    fpr, tpr, tresholds = metrics.roc_curve(true, pred)\n",
    "    auc = metrics.auc(fpr,tpr)\n",
    "    \n",
    "    return auc\n",
    "    \n",
    "\n",
    "def variable_on_cpu(name, shape, initializer):\n",
    "    \"\"\"Helper to create a Variable stored on CPU memory.\n",
    "    Args:\n",
    "    name: name of the variable\n",
    "    shape: list of ints\n",
    "    initializer: initializer for Variable\n",
    "    Returns:\n",
    "    Variable Tensor\n",
    "    \"\"\"\n",
    "    with tf.device('/cpu:0'):\n",
    "        dtype = tf.float32\n",
    "        var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype)\n",
    "    return var\n",
    "\n",
    "def variable_with_weight_decay(name, shape, stddev, wd):\n",
    "    \"\"\"Helper to create an initialized Variable with weight decay.\n",
    "    Note that the Variable is initialized with a truncated normal distribution.\n",
    "    A weight decay is added only if one is specified.\n",
    "    Args:\n",
    "    name: name of the variable\n",
    "    shape: list of ints\n",
    "    stddev: standard deviation of a truncated Gaussian\n",
    "    wd: add L2Loss weight decay multiplied by this float. If None, weight\n",
    "        decay is not added for this Variable.\n",
    "    Returns:\n",
    "    Variable Tensor\n",
    "    \"\"\"\n",
    "    dtype = tf.float32\n",
    "    var = variable_on_cpu(name, shape, \n",
    "                           tf.truncated_normal_initializer(stddev=stddev, dtype=dtype))\n",
    "    if wd is not None:\n",
    "        w_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "        tf.add_to_collection('losses', w_decay)\n",
    "    return var\n",
    "\n",
    "def par_conv_split_duo(x, keep_prob, motifs_1, motifs_2, motif_length_1, motif_length_2, stdev, stdev_out, w_decay, \n",
    "                   w_out_decay, pooling=1, num_classes=2, padding=False, extra_layer=False):\n",
    "    \n",
    "    x_image = tf.reshape(x, [-1,1,50,4])\n",
    "    padding = motif_length//2 if padding is True else 0\n",
    "    pool_list = []\n",
    "    par_conv_1 = math.ceil(50/motif_length_1)\n",
    "    par_conv_2 = math.ceil(50/motif_length_2)\n",
    "    for conv in range(par_conv_1):\n",
    "        i_start = (50-(conv+1)*motif_length_1)-padding\n",
    "        i_end = (50-conv*motif_length_1)+padding\n",
    "        if i_start < 0:\n",
    "            i_start = 0\n",
    "        x_sub_image_length = len(range(i_start,i_end))\n",
    "        x_sub_image = x_image[:,:,i_start:i_end,:]\n",
    "        with tf.variable_scope('conv{}_sh'.format(conv)) as scope:    \n",
    "            kernel = variable_with_weight_decay('weights',\n",
    "                                             shape=[1, motif_length_1, 4, motifs_1],\n",
    "                                             stddev=stdev,\n",
    "                                             wd=w_decay)\n",
    "            conv_unit = tf.nn.conv2d(x_sub_image, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = variable_on_cpu('biases', [motifs_1], tf.constant_initializer(0.001))\n",
    "            pre_activation = tf.nn.bias_add(conv_unit, biases)\n",
    "            conv_act = tf.nn.relu(pre_activation, name=scope.name)\n",
    "            \n",
    "        with tf.variable_scope('pool{}'.format(conv)) as scope:\n",
    "            if pooling in [-1,2]:\n",
    "                max_pool_unit = tf.nn.max_pool(conv_act, ksize=[1, 1, x_sub_image_length, 1], \n",
    "                            strides=[1, 1, x_sub_image_length, 1], padding='SAME')\n",
    "                max_pool_flat = tf.reshape(max_pool_unit, [-1, motifs_1])\n",
    "                pool_list.append(max_pool_flat)\n",
    "            if pooling in [1,2]:\n",
    "                avg_pool_unit = tf.nn.avg_pool(conv_act, ksize=[1, 1, x_sub_image_length, 1], \n",
    "                                               strides=[1, 1, x_sub_image_length, 1], padding='SAME')\n",
    "                avg_pool_flat = tf.reshape(avg_pool_unit, [-1, motifs_1])\n",
    "                pool_list.append(avg_pool_flat)\n",
    "            \n",
    "    for conv in range(par_conv_2):\n",
    "        i_start = (50-(conv+1)*motif_length_2)-padding\n",
    "        i_end = (50-conv*motif_length_2)+padding\n",
    "        if i_start < 0:\n",
    "            i_start = 0\n",
    "        x_sub_image_length = len(range(i_start,i_end))\n",
    "        x_sub_image = x_image[:,:,i_start:i_end,:]\n",
    "        with tf.variable_scope('conv{}_lo'.format(conv)) as scope:    \n",
    "            kernel = variable_with_weight_decay('weights',\n",
    "                                             shape=[1, motif_length_2, 4, motifs_2],\n",
    "                                             stddev=stdev,\n",
    "                                             wd=w_decay)\n",
    "            conv_unit = tf.nn.conv2d(x_sub_image, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = variable_on_cpu('biases', [motifs_2], tf.constant_initializer(0.001))\n",
    "            pre_activation = tf.nn.bias_add(conv_unit, biases)\n",
    "            conv_act = tf.nn.relu(pre_activation, name=scope.name)\n",
    "        \n",
    "\n",
    "        with tf.variable_scope('pool{}'.format(conv)) as scope:\n",
    "            if pooling in [-1,2]:\n",
    "                max_pool_unit = tf.nn.max_pool(conv_act, ksize=[1, 1, x_sub_image_length, 1], \n",
    "                            strides=[1, 1, x_sub_image_length, 1], padding='SAME')\n",
    "                max_pool_flat = tf.reshape(max_pool_unit, [-1, motifs_2])\n",
    "                pool_list.append(max_pool_flat)\n",
    "            if pooling in [1,2]:\n",
    "                avg_pool_unit = tf.nn.avg_pool(conv_act, ksize=[1, 1, x_sub_image_length, 1], \n",
    "                                               strides=[1, 1, x_sub_image_length, 1], padding='SAME')\n",
    "                avg_pool_flat = tf.reshape(avg_pool_unit, [-1, motifs_2])\n",
    "                pool_list.append(avg_pool_flat)\n",
    "            \n",
    "    num_pool_values = ((motifs_1*par_conv_1)+(motifs_2*par_conv_2))*abs(pooling)\n",
    "    layer2 = tf.reshape(tf.concat(pool_list, 1), [-1, num_pool_values])\n",
    "    \n",
    "    if extra_layer is True:\n",
    "        with tf.variable_scope('fully_connected'):\n",
    "            weights = variable_with_weight_decay('weights', shape=[num_pool_values, num_pool_values],\n",
    "                                              stddev=stdev_out, wd=w_out_decay)\n",
    "            biases = variable_on_cpu('biases', num_pool_values, tf.constant_initializer(0.1))\n",
    "            layer2 = tf.nn.relu(tf.matmul(layer2, weights) + biases)\n",
    "\n",
    "    with tf.variable_scope('out') as scope:\n",
    "        weights = variable_with_weight_decay('weights', shape=[num_pool_values, num_classes],\n",
    "                                          stddev=stdev_out, wd=w_out_decay)\n",
    "        biases = variable_on_cpu('biases', num_classes, tf.constant_initializer(0))\n",
    "        softmax_linear = tf.nn.sigmoid(tf.matmul(layer2, weights) + biases)\n",
    "\n",
    "        return softmax_linear\n",
    "\n",
    "def par_conv_split(x, keep_prob, motifs, motif_length, stdev, stdev_out, w_decay, \n",
    "                   w_out_decay, pooling=1, num_classes=2, padding=False, extra_layer=False):\n",
    "    \n",
    "    x_image = tf.reshape(x, [-1,1,50,4])\n",
    "    padding = motif_length//2 if padding is True else 0\n",
    "    pool_list = []\n",
    "    par_conv = math.ceil(50/motif_length)\n",
    "    for conv in range(par_conv):\n",
    "        i_start = (50-(conv+1)*motif_length)-padding\n",
    "        i_end = (50-conv*motif_length)+padding\n",
    "        if i_start < 0:\n",
    "            i_start = 0\n",
    "        x_sub_image_length = len(range(i_start,i_end))\n",
    "        x_sub_image = x_image[:,:,i_start:i_end,:]\n",
    "        with tf.variable_scope('conv{}'.format(conv)) as scope:    \n",
    "            kernel = variable_with_weight_decay('weights',\n",
    "                                             shape=[1, motif_length, 4, motifs],\n",
    "                                             stddev=stdev,\n",
    "                                             wd=w_decay)\n",
    "            conv_unit = tf.nn.conv2d(x_sub_image, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = variable_on_cpu('biases', [motifs], tf.constant_initializer(0.001))\n",
    "            pre_activation = tf.nn.bias_add(conv_unit, biases)\n",
    "            conv_act = tf.nn.relu(pre_activation, name=scope.name)\n",
    "\n",
    "        with tf.variable_scope('pool{}'.format(conv)) as scope:\n",
    "            if pooling in [-1,2]:\n",
    "                max_pool_unit = tf.nn.max_pool(conv_act, ksize=[1, 1, x_sub_image_length, 1], \n",
    "                            strides=[1, 1, x_sub_image_length, 1], padding='SAME')\n",
    "                max_pool_flat = tf.reshape(max_pool_unit, [-1, motifs])\n",
    "                pool_list.append(max_pool_flat)\n",
    "            if pooling in [1,2]:\n",
    "                avg_pool_unit = tf.nn.avg_pool(conv_act, ksize=[1, 1, x_sub_image_length, 1], \n",
    "                                               strides=[1, 1, x_sub_image_length, 1], padding='SAME')\n",
    "                avg_pool_flat = tf.reshape(avg_pool_unit, [-1, motifs])\n",
    "                pool_list.append(avg_pool_flat)\n",
    "                \n",
    "    num_pool_values = motifs*par_conv*abs(pooling)\n",
    "    layer2 = tf.reshape(tf.concat(pool_list, 1), [-1, num_pool_values])\n",
    "    \n",
    "    if extra_layer is True:\n",
    "        with tf.variable_scope('fully_connected'):\n",
    "            weights = variable_with_weight_decay('weights', shape=[num_pool_values, num_pool_values],\n",
    "                                              stddev=stdev_out, wd=w_out_decay)\n",
    "            biases = variable_on_cpu('biases', motifs*par_conv, tf.constant_initializer(0.1))\n",
    "            layer2 = tf.nn.relu(tf.matmul(layer2, weights) + biases)\n",
    "\n",
    "    with tf.variable_scope('out') as scope:\n",
    "        weights = variable_with_weight_decay('weights', shape=[num_pool_values, num_classes],\n",
    "                                          stddev=stdev_out, wd=w_out_decay)\n",
    "        biases = variable_on_cpu('biases', num_classes, tf.constant_initializer(0))\n",
    "        softmax_linear = tf.nn.sigmoid(tf.matmul(layer2, weights) + biases)\n",
    "\n",
    "        return softmax_linear\n",
    "\n",
    "def conv_network(x, keep_prob, motifs, motif_length, stdev, stdev_out, w_decay, \n",
    "                 w_out_decay, single_pool=True, pooling=1, num_classes=2, padding=False,\n",
    "                 extra_layer=False):\n",
    "    \n",
    "    x_image = tf.reshape(x, [-1,1,50,4])\n",
    "    padding = motif_length//2 if padding is True else 0\n",
    "    motifs = math.ceil(50/motif_length)*motifs\n",
    "    if single_pool:\n",
    "        num_pool_values = motifs\n",
    "        pool_stride = 50\n",
    "    else:\n",
    "        num_pool_values = (motifs*(math.ceil(50/motif_length)))\n",
    "        pool_stride = motif_length\n",
    "\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        kernel = variable_with_weight_decay('weights',\n",
    "                                             shape=[1, motif_length, 4, motifs],\n",
    "                                             stddev=stdev,\n",
    "                                             wd=w_decay)\n",
    "        conv = tf.nn.conv2d(x_image, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = variable_on_cpu('biases', [motifs], tf.constant_initializer(0))\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv1 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "        \n",
    "        if pooling in [-1,2]:\n",
    "            max_pool_unit = tf.nn.max_pool(conv_act, ksize=[1, 1, pool_stride, 1], \n",
    "                        strides=[1, 1, pool_stride, 1], padding='SAME')\n",
    "            max_pool_flat = tf.reshape(max_pool_unit, [-1, num_pool_values])\n",
    "        if pooling in [1,2]:\n",
    "            avg_pool_unit = tf.nn.avg_pool(conv_act, ksize=[1, 1, pool_stride, 1], \n",
    "                                           strides=[1, 1, pool_stride, 1], padding='SAME')\n",
    "            avg_pool_flat = tf.reshape(avg_pool_unit, [-1, num_pool_values])\n",
    "            \n",
    "        num_pool_values = num_pool_values*abs(pooling)\n",
    "        layer2 = tf.reshape(tf.concat([max_pool_flat, avg_pool_flat], axis=0), [-1, num_pool_values])\n",
    "    \n",
    "    if extra_layer is True:\n",
    "        with tf.variable_scope('fully_connected'):\n",
    "            weights = variable_with_weight_decay('weights', shape=[num_pool_values, num_pool_values],\n",
    "                                              stddev=stdev_out, wd=w_out_decay)\n",
    "            biases = variable_on_cpu('biases', num_pool_values, tf.constant_initializer(0.1))\n",
    "            fully_connected = tf.nn.relu(tf.matmul(layer2, weights) + biases)\n",
    "            layer2 = tf.nn.dropout(fully_connected, keep_prob)\n",
    "\n",
    "    with tf.variable_scope('out') as scope:\n",
    "        weights = variable_with_weight_decay('weights', [num_pool_values, num_classes],\n",
    "                                              stddev=stdev_out, wd=w_out_decay)\n",
    "        biases = variable_on_cpu('biases', [num_classes],\n",
    "                                  tf.constant_initializer(0))\n",
    "        softmax_linear = tf.sigmoid(tf.matmul(layer2, weights) + biases)\n",
    "\n",
    "\n",
    "    return softmax_linear\n",
    "    \n",
    "\n",
    "def SelectModel(model_label, x, keep_prob, motifs, motif_length, stdev, stdev_out, w_decay, \n",
    "                w_out_decay, pooling, num_classes=2, padding=False, extra_layer=False):\n",
    "    \n",
    "    if model_label == \"MS1\":\n",
    "        model = conv_network(x, keep_prob, motifs[0], motif_length[0], stdev, stdev_out, w_decay,\n",
    "                             w_out_decay, False, pooling, num_classes, padding, extra_layer)\n",
    "    if model_label == \"MS2\":\n",
    "        model = conv_network(x, keep_prob, motifs[0], motif_length[0], stdev, stdev_out, w_decay, \n",
    "                             w_out_decay, True, pooling, num_classes, padding, extra_layer)\n",
    "    \n",
    "    if model_label == \"MS3\":\n",
    "        model = par_conv_split(x, keep_prob, motifs[0], motif_length[0], stdev, stdev_out, w_decay,\n",
    "                               w_out_decay, pooling, num_classes, padding, extra_layer)\n",
    "    if model_label == \"MS4\":\n",
    "        model = par_conv_split_duo(x, keep_prob, motifs[0], motifs[1], motif_length[0], motif_length[1], stdev, stdev_out, w_decay,\n",
    "                               w_out_decay, pooling, num_classes, padding, extra_layer)\n",
    "        \n",
    "    return model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
